{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "We continue our study of classification by examining a popular model known as logistic regression. It belongs to the class of probabilistic models and here we model the probability that a feature vector $\\phi(\\mathbf{x})$ belongs to a given class.\n",
    "\n",
    "We consider the two class problem where given an input vector, $\\mathbf{x}$, we seek to assign it to one of two classes $\\mathcal{C}_1$ or $\\mathcal{C}_2$.\n",
    "\n",
    "We model the probability\n",
    "\n",
    "$\\begin{equation}\n",
    "p\\big(\\mathcal{C}_1\\big|\\phi(\\mathbf{x})\\big)=\\sigma\\big(\\mathbf{w}^T\\phi(\\mathbf{x})\\big)\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "Where $\\sigma(.)$ is the logistic sigmoid function defined as\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "\\sigma(a)=\\frac{1}{1+e^{-a}}\n",
    "\\end{equation}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "a = np.linspace(-10, 10, 100)\n",
    "plt.plot(a, 1 / (1 + np.exp(-a)))\n",
    "plt.grid(True)\n",
    "plt.xlabel(r'$a$', fontsize=14)\n",
    "plt.ylabel(r'$\\sigma(a)$', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the probabilities must sum to one, $p\\big(\\mathcal{C}_2\\big|\\phi(\\mathbf{x})\\big)=1-p\\big(\\mathcal{C}_1\\big|\\phi(\\mathbf{x})\\big)$\n",
    "\n",
    "For a given data set $\\{\\mathbf{x}_n, t_n\\}$ where $t_n\\in\\{0, 1\\}$, we estimate the parameters by maximum likelihood.\n",
    "\n",
    "### Maximum Likelihood Detour\n",
    "For a single data point $\\mathbf{x}_n$, let $y_n = \\sigma\\big(\\mathbf{w}^T\\phi(\\mathbf{x}_n)\\big)$. \n",
    "\n",
    "We can write\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "p(t_n|\\mathbf{w},\\mathbf{x}_n)=\\left\\{ \\begin{array}{ll}\n",
    "y_n & \\textrm{ $t_n = 1$}\\\\\n",
    "1-y_n & \\textrm{$t_n=0$}\n",
    "\\end{array} \\right.\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "This can be written compactly as \n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "p(t_n|\\mathbf{w},\\mathbf{x}_n)=y_n^{t_n}(1-y_n)^{1-t_n}\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "The likelihood function is the \n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "p(\\mathbf{t}|\\mathbf{w}) = \\prod_{n=1}^Ny_n^{t_n}(1-y_n)^{1-t_n}\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "We seek the $\\mathbf{w}$ that maximizes the likelihood. This is equivalent to maximizing the logarithm of the likelihood or minimizing the negative logarithm of the likelihood.\n",
    "\n",
    "We define the error function as the negative logarithm of the likelihood\n",
    "\n",
    "We have\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "E(\\mathbf{w}) = -\\log p(\\mathbf{t}|\\mathbf{w})=-\\sum_{n=1}^N\\{t_n\\ln y_n + (1-t_n)\\ln(1-y_n)\\}\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "This is known as the cross-entropy error function.\n",
    "\n",
    "To proceed, we compute the gradient of this error function. We get \n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "\\nabla E(\\mathbf{w})=\\sum_{n=1}^N(y_n-t_n)\\phi(\\mathbf{x}_n) \n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "We can proceed by stochastic gradient descent where we have\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "\\mathbf{w}^{(\\tau + 1)}=\\mathbf{w}^{(\\tau)} - \\eta\\nabla E_n(\\mathbf{w}) \n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "where\n",
    "\n",
    "$E_n(\\mathbf{w}) =(y_n-t_n)\\phi(\\mathbf{x}_n) $ \n",
    "\n",
    "### Example\n",
    "\n",
    "Now let us try things out on our favourite Gaussian blob data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class = 2\n",
    "num_per_class = 20\n",
    "var = 0.1\n",
    "means = [[1,1],[-1,-1]]\n",
    "cov = [[var, 0], [0, var]] \n",
    "X = np.array([])\n",
    "y = []\n",
    "class_color = ['b','r']\n",
    "\n",
    "for class_index in range(num_class):\n",
    "    class_data = np.random.multivariate_normal(means[class_index],\n",
    "                                               cov, \n",
    "                                               num_per_class)\n",
    "    X = np.vstack([X, class_data]) if X.size else class_data\n",
    "    y = np.concatenate((y, np.ones(num_per_class) * class_index))\n",
    "    plt.plot(class_data[:, 0], \n",
    "             class_data[:,1], \n",
    "             class_color[class_index] + 'o')\n",
    "    plt.xlim([-2, 2])\n",
    "    plt.ylim([-2, 2])\n",
    "    plt.xlabel(r'$x_1$', fontsize=14)\n",
    "    plt.ylabel(r'$x_2$', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will learn the weight vector by stochastic gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy_gradient(input_vector, weight, target):\n",
    "    '''Compute the gradient of the cross entropy error function\n",
    "    Args:\n",
    "        input_vector: the input vector x\n",
    "        weight: the weight vector w\n",
    "        target: the target value 1 or 0\n",
    "    Returns:\n",
    "        gradient: the gradient at the input vector\n",
    "    '''\n",
    "    yn = 1 / (1 + np.exp(-np.dot(weight, input_vector)))\n",
    "    return (yn - target)*input_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine the decision boundary we need to have a decision rule. A reasonable rule is to classify an input vector as belonging to $\\mathcal{C}_1$ if $p\\big(\\mathcal{C}_1\\big|\\phi(\\mathbf{x})\\big) > 0.5$. Therefore the decision boundary is given by\n",
    "\n",
    "$\\begin{equation}\n",
    "\\sigma\\big(\\mathbf{w}^T\\phi(\\mathbf{x})\\big)=0.5\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "To plot this we need the inverse function of the logistic sigmoid which is \n",
    "\n",
    "$\\begin{equation}\n",
    "a = \\ln\\Big(\\frac{\\sigma}{1-\\sigma}\\Big)\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "We plot the line $\\mathbf{w}^T\\phi(\\mathbf{x})=0$. In this case  $\\phi(x)=x$\n",
    "\n",
    "Let's run the algorithm starting at $\\mathbf{w} = [1, -1]$. We see that initially some examples are misclassified.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# straight line\n",
    "x1 = np.linspace(-2, 2, 100)\n",
    "x2 = x1\n",
    "for class_index in range(num_class):\n",
    "    plt.plot(X[y==class_index, 0], \n",
    "             X[y==class_index, 1], \n",
    "             class_color[class_index] + 'o')\n",
    "    plt.plot(x1, x2)\n",
    "    plt.xlim([-2, 2])\n",
    "    plt.ylim([-2, 2])\n",
    "    plt.xlabel(r'$x_1$', fontsize=14)\n",
    "    plt.xlabel(r'$x_2$', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_vector = np.array([1, -1])\n",
    "\n",
    "for index in range(X.shape[0]):\n",
    "    weight_vector = weight_vector - cross_entropy_gradient(X[index, :], \n",
    "                                                        weight_vector, \n",
    "                                                        y[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can now plot the line corresponding to this learned weight vector and verify that it separates the classes correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the line\n",
    "x2 = -x1 * (weight_vector[0] / weight_vector[1]) \n",
    "\n",
    "for class_index in range(num_class):\n",
    "    plt.plot(X[y==class_index, 0], \n",
    "             X[y==class_index, 1], \n",
    "             class_color[class_index] + 'o')\n",
    "    plt.plot(x1, x2)\n",
    "    plt.xlim([-2, 2])\n",
    "    plt.ylim([-2, 2])\n",
    "    plt.xlabel(r'$x_1$', fontsize=14)\n",
    "    plt.xlabel(r'$x_2$', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the perceptron, logistic regression can be readily extended to the multiclass problem. See Bishop section 4.3.4.\n",
    "\n",
    "## Reference\n",
    "This notebook is based on the description of the perceptron algorithm in section 4.3.2 in Bishop's book - [Pattern Recognition and Machine Learning](https://www.springer.com/gp/book/9780387310732)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
