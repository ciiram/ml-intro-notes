{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "Sentiment analysis is a machine learning application which aims to infer the sentiment associated with a piece of text. This application could be useful to businesses trying to gauge customer satisfaction, sentiment on a certain issue on social media etc.\n",
    "\n",
    "We will attempt to use the perceptron and logistic regression to automatically analyse the sentiment of movie reviews. The [data](https://web.stanford.edu/class/cs221/assignments/sentiment/index.html) are from [Percy Liang's](https://cs.stanford.edu/~pliang/) course.\n",
    "\n",
    "Initially let us look at a simple data set of 4 reviews of a course at the univesity\n",
    "1. easy (+ve)\n",
    "1. very informative (+ve)\n",
    "1. useless stuff (-ve)\n",
    "1. hard (-ve)\n",
    "\n",
    "Each of the reviews is a string which we treat as our input $x$. A useful feature $\\phi(x)$ would be the counts of each of the six words ['easy', 'very', 'informative', 'useless', 'stuff', 'hard'] in each review. For example, $\\phi('easy') = [1, 0, 0, 0, 0, 0]$.\n",
    "\n",
    "Let's try and train a perceptron on this dataset. We need a function to create the feature vector from the input string data.\n",
    "\n",
    "\n",
    "We will use the `collections` library. Let's play around with this small data set before treating the movies data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "data = ['easy', 'very informative', 'useless stuff', 'hard']\n",
    "\n",
    "# obtain the counts of words in each string\n",
    "counts = []\n",
    "for string in data:\n",
    "    str_split_cnt = Counter(string.split())\n",
    "    counts.append(str_split_cnt)\n",
    "    \n",
    "    \n",
    "# get the dictionary of all words\n",
    "cum_counts = Counter()\n",
    "for count in counts:\n",
    "    cum_counts += count\n",
    "words = list(cum_counts.keys())\n",
    "\n",
    "print(words)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define a function that maps the string count to the feature vectors of counts for all words in the dictionary. This vector will often be very sparse and more efficient methods exist. Here we generate these vectors for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def word_count(string_count, word_list):\n",
    "    ''' Compute a feature vector from the string word count\n",
    "    Args:\n",
    "        string_count: a Counter with words and counts\n",
    "        word_list: all words\n",
    "    Returns:\n",
    "       word_count_vector\n",
    "    '''\n",
    "    word_count_vector = np.zeros(len(word_list))\n",
    "    for k in string_count.keys():\n",
    "       word_count_vector[word_list.index(k)] = string_count[k] \n",
    "    \n",
    "    return word_count_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_count(counts[3], words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now move to the movies data set. First we download the data and extract the training strings and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_train = open('../data/polarity.train', 'r')\n",
    "\n",
    "# read the first five lines, strip out the final newline\n",
    "for _ in range(5):\n",
    "    print(reviews_train.readline().strip())\n",
    "reviews_train.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now obtain the entire training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_train = open('../data/polarity.train', 'r', encoding=\"utf-8\", errors='ignore')\n",
    "labels = []\n",
    "word_counts = []\n",
    "for curr_line in reviews_train:\n",
    "    curr_review = curr_line.strip()[3:-2]\n",
    "    curr_label = int(curr_line.strip()[:2])\n",
    "    labels.append(curr_label)\n",
    "    word_counts.append(Counter(curr_review.split()))\n",
    "    print(curr_label, curr_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_counts = Counter()\n",
    "for count in word_counts:\n",
    "    cum_counts += count\n",
    "words = list(cum_counts.keys())\n",
    "\n",
    "print(words)\n",
    "print(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many words?\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.zeros((len(word_counts), len(words)))\n",
    "y = np.array(labels)\n",
    "for index, count in enumerate(word_counts):\n",
    "    X[index, :] = word_count(count, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the models\n",
    "We will use scikit learn to train both the perceptron and logistic regression classifier and compare their performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split # to obtain the train, validation and test split\n",
    "from sklearn.linear_model import Perceptron, LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=.2)# 80% for training\n",
    "\n",
    "perceptron_clf = Perceptron()\n",
    "lr_clf = LogisticRegression(random_state=0, solver='lbfgs')\n",
    "\n",
    "models = [perceptron_clf, lr_clf]\n",
    "model_labels = ['Perceptron', 'Logistic Regression']\n",
    "model_train_score = np.zeros(len(models))\n",
    "model_test_score = np.zeros(len(models))\n",
    "\n",
    "for index, model in enumerate(models):\n",
    "    model.fit(X_train, y_train)\n",
    "    model_train_score[index] = model.score(X_train, y_train)\n",
    "    model_test_score[index] = model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, model in enumerate(models):\n",
    "    print(model_labels[index], 'Training accuracy:', model_train_score[index])\n",
    "    print(model_labels[index], 'Test accuracy:', model_test_score[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
